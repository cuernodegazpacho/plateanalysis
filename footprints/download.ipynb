{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2da0894",
   "metadata": {},
   "source": [
    "# Downloads data from APPLAUSE  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a333b774",
   "metadata": {},
   "source": [
    "##  WORK IN PROGRESS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529758ee",
   "metadata": {},
   "source": [
    "This script automatically downloads from the APPLAUSE archive, all data necessay to run the analysis scripts on a plate sequence.\n",
    "\n",
    "For each entry (plate ID) in a sequence, we need  to download two kinds of data: plate scans, and source tables. Analysis requires two scans and four source tables per pair of exposures:\n",
    "\n",
    "- plate scan 1 \n",
    "- plate scan 2\n",
    "- sources 1\n",
    "- sources calib 1\n",
    "- sources 2\n",
    "- sources calib 2\n",
    "\n",
    "Data is downloaded only when not already physically present in the data directory (DATADIR path in file *settings.py*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f0d5514",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from importlib import reload\n",
    "from urllib.parse import urlsplit\n",
    "import requests\n",
    "import json\n",
    "\n",
    "import pyvo as vo\n",
    "\n",
    "from astropy.io import fits\n",
    "from astropy.utils.data import download_file\n",
    "from astropy.table import Table\n",
    "\n",
    "from applause_token import token\n",
    "from settings import DATAPATH, get_parameters, get_table_sources, current_dataset, images, fname, sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba93ab6d",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25ff233f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_scan(table, plate_id, test=False):\n",
    "    \n",
    "    print(\"Downloading scan for plate: \", plate_id' \"...\")\n",
    "\n",
    "    # get URL for this plate ID \n",
    "    mask = table['plate_id'] == plate_id\n",
    "    table_1 = table[mask]\n",
    "    url_1 = table_1['filename_scan'][0]\n",
    "    \n",
    "    # get file name from URL\n",
    "    parsed_url = urlsplit(url_1)\n",
    "    filename = parsed_url.path.split('/')[-1]\n",
    "    \n",
    "    # if file already exists in data storage, bail out\n",
    "    file_path = Path(os.path.join(DATAPATH, filename))\n",
    "    if file_path.is_file():\n",
    "        print(\"Image scan for plate: \", plate_id, \" already in storage.\")\n",
    "        return\n",
    "\n",
    "    # Download the file and get the local filename/path\n",
    "    local_file_path = download_file(url_1)\n",
    "    if test:\n",
    "        print(f\"File downloaded to: {local_file_path}\")\n",
    "\n",
    "    # Move and rename file into data directory\n",
    "    new_path = shutil.move(local_file_path, os.path.join(DATAPATH, filename))\n",
    "    print(\"Image scan for plate: \", plate_id, \" downloaded and written to:  \", new_path)\n",
    "\n",
    "    # TEST: open the local FITS file using the obtained path\n",
    "    if test:\n",
    "        with fits.open(new_path) as hdul:\n",
    "            # Access the header information or data\n",
    "            header_info = hdul[0].header\n",
    "            print(f\"TEST: telescope used: {header_info.get('TELESCOP')}\")\n",
    "            print(f\"TEST: date: {header_info.get('DATE-AVG')}\")\n",
    "\n",
    "    # once image is successfully downloaded, we need to update the 'images.json' \n",
    "    # dictionary with the new association \"plate id: file name\" entry.\n",
    "    images[str(plate_id)] = filename        \n",
    "            \n",
    "    try:\n",
    "        json_file = open('images.json', 'w')\n",
    "        json.dump(images, json_file, indent=4)\n",
    "    except IOError as e:\n",
    "        print(f\"Error writing to file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b883343e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_sources_table(plate_id, calib=False):\n",
    "    '''\n",
    "    This function requires that a 'token' variable be present in the namespace.\n",
    "\n",
    "    The usual way to do this is to create a 1-line .py import file named \n",
    "    'applause_token'. The file will define the variable with the token definition \n",
    "    you got from APPLAUSE (look for the 'API token' entry in the dropdown menu \n",
    "    from your login name in the APPLAUSE web page). Something like:\n",
    "    \n",
    "    token = '2a2223453fdf3eq89aweafs6da05415hhh1d4369193e'\n",
    "    \n",
    "    '''\n",
    "    # printout helper\n",
    "    cal_suffix = \"\"\n",
    "    if calib:\n",
    "        cal_suffix = \"_calib\"\n",
    "\n",
    "    # If table already exists in data storage, bail out.\n",
    "    # We check just one table of the set of four, assuming \n",
    "    # that it is an error if one or more of the four tables\n",
    "    # are missing.\n",
    "    table_name = fname(get_table_sources(plate_id, calib=calib))\n",
    "    file_path = Path(table_name)\n",
    "    if file_path.is_file():\n",
    "        print(\"Table sources\" + cal_suffix + \" for plate: \", plate_id, \" already in storage.\")\n",
    "        return\n",
    "\n",
    "    print(\"Downloading table sources\" + cal_suffix + \" for plate: \", plate_id, \"...\")\n",
    "\n",
    "    # setup\n",
    "    name = 'APPLAUSE',\n",
    "    url = 'https://www.plate-archive.org/tap'\n",
    "    ap_token = 'Token ' + token\n",
    "\n",
    "    # assemble query string\n",
    "    qstr = 'SELECT * FROM applause_dr4.source'\n",
    "    if calib:\n",
    "        qstr = qstr + '_calib'\n",
    "    qstr = qstr + ' WHERE plate_id = ' + str(plate_id) + ' ORDER BY source_id'\n",
    "\n",
    "#     print('\\npyvo version %s \\n' % vo.__version__)\n",
    "#     print('TAP service %s \\n' % name)\n",
    "\n",
    "    # session mechanics\n",
    "    tap_session = requests.Session()\n",
    "    tap_session.headers['Authorization'] = ap_token\n",
    "\n",
    "    tap_service = vo.dal.TAPService(url, session=tap_session)\n",
    "\n",
    "    lang = 'PostgreSQL'\n",
    "\n",
    "    job = tap_service.submit_job(qstr, language=lang)\n",
    "    job.run()\n",
    "\n",
    "    job.wait(phases=[\"COMPLETED\", \"ERROR\", \"ABORTED\"], timeout=7200000.) # 2-hr queue\n",
    "\n",
    "    try:\n",
    "        job.raise_if_error()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return\n",
    "    \n",
    "    results = job.fetch_result()\n",
    "\n",
    "    # save an astropy table in csv format (for backwards compatibility)\n",
    "    result_table = results.to_table()\n",
    "    \n",
    "    table_name = get_table_sources(plate_id)\n",
    "    file_name = fname(table_name)\n",
    "    results.write(file_name, format='csv', overwrite=True)\n",
    "    \n",
    "    print(\"Table\", table_name, + \" downloaded and saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b10c24f",
   "metadata": {},
   "source": [
    "## Define catalog and sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f84389e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9534, 9535, 9536, 9537, 9538, 9539, 9540, 9543]\n"
     ]
    }
   ],
   "source": [
    "catalog_applause = Table.read('footprints_5.csv', format='ascii.csv')\n",
    "\n",
    "sequence = sequences['seq 11']\n",
    "print(sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1a3d63",
   "metadata": {},
   "source": [
    "## Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd133675",
   "metadata": {},
   "outputs": [
    {
     "ename": "DALQueryError",
     "evalue": "Query Error: <No useful error from server>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDALQueryError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# for plate_id in sequence:\u001b[39;00m\n\u001b[1;32m      2\u001b[0m     \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#     download_scan(catalog_applause, plate_id)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#     download_sources_table(plate_id)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#     download_sources_table(plate_id, calib=True)\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[43mdownload_sources_table\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m9534\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 55\u001b[0m, in \u001b[0;36mdownload_sources_table\u001b[0;34m(plate_id, calib)\u001b[0m\n\u001b[1;32m     51\u001b[0m job\u001b[38;5;241m.\u001b[39mrun()\n\u001b[1;32m     53\u001b[0m job\u001b[38;5;241m.\u001b[39mwait(phases\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCOMPLETED\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mERROR\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mABORTED\u001b[39m\u001b[38;5;124m\"\u001b[39m], timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m70.\u001b[39m)\n\u001b[0;32m---> 55\u001b[0m \u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_if_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m results \u001b[38;5;241m=\u001b[39m job\u001b[38;5;241m.\u001b[39mfetch_result()\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# save an astropy table in csv format (for backwards compatibility)\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/software/miniconda3/envs/astro/lib/python3.10/site-packages/pyvo/dal/tap.py:1026\u001b[0m, in \u001b[0;36mAsyncTAPJob.raise_if_error\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_job\u001b[38;5;241m.\u001b[39merrorsummary\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m   1025\u001b[0m msg \u001b[38;5;241m=\u001b[39m msg \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<No useful error from server>\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m DALQueryError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuery Error: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m msg, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl)\n",
      "\u001b[0;31mDALQueryError\u001b[0m: Query Error: <No useful error from server>"
     ]
    }
   ],
   "source": [
    "# for plate_id in sequence:\n",
    "    \n",
    "#     download_scan(catalog_applause, plate_id)\n",
    "#     download_sources_table(plate_id)\n",
    "#     download_sources_table(plate_id, calib=True)\n",
    "\n",
    "\n",
    "\n",
    "# debug --  applause seems to be broken....\n",
    "\n",
    "download_sources_table(9534)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecaee42f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
